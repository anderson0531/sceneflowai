import { VisualReference } from '@/types/visionReferences'

export type SceneSegmentStatus = 'DRAFT' | 'READY' | 'GENERATING' | 'COMPLETE' | 'UPLOADED' | 'ERROR'

// ============================================================================
// Scene Context Types (AI-enhanced scene details)
// ============================================================================

/**
 * Enhanced scene context generated by AI
 * Provides storytelling context for the user without being a production asset
 */
export interface SceneContext {
  /** The narrative beat this scene represents (e.g., "Inciting incident", "Rising action") */
  beat?: string
  /** How characters evolve in this scene (e.g., "Hero realizes their weakness") */
  characterArc?: string
  /** Thematic elements explored (e.g., "Sacrifice vs self-preservation") */
  thematicContext?: string
  /** ISO timestamp of when context was generated */
  generatedAt?: string
}

// ============================================================================
// Keyframe State Machine Types
// ============================================================================

/**
 * Transition type between segments
 * CONTINUE: End frame of previous segment becomes start frame (visual continuity)
 * CUT: Fresh start frame generated (scene change, location change, etc.)
 */
export type TransitionType = 'CONTINUE' | 'CUT'

/**
 * Anchor status for the Keyframe State Machine
 * Tracks the progression of frame generation for a segment
 */
export type AnchorStatus = 
  | 'pending'           // No frames generated yet
  | 'start-locked'      // Start frame is ready
  | 'end-pending'       // Start locked, waiting for end frame
  | 'fully-anchored'    // Both start and end frames ready for FTV

/**
 * Action type classification for inverse proportionality weights
 */
export type ActionType = 
  | 'static'           // Character holds pose, no movement
  | 'subtle'           // Micro-expressions, eye movement, breathing
  | 'speaking'         // Dialogue delivery, mouth movement
  | 'gesture'          // Hand gestures, head turns
  | 'movement'         // Walking, repositioning
  | 'action'           // Running, fighting, physical activity
  | 'transformation'   // Major change (costume, lighting, time skip)

/**
 * Frame anchor data for Start/End keyframes
 */
export interface FrameAnchor {
  url: string
  description: string
  generatedAt: string
  actionType?: ActionType
}

// Establishing Shot Types - Video-focused modes for narration backdrops
// DEPRECATED: 'scale-switch' | 'living-painting' | 'b-roll-cutaway' (photo-based tricks)
// NEW: Video generation modes that work with AI video clips
export type EstablishingShotType = 
  | 'single-shot'    // One continuous video for entire narration (loops if needed)
  | 'beat-matched'   // AI splits narration into visual beats, generates multiple segments
  | 'manual-cuts'    // User defines cut points manually
  // Legacy support (deprecated but still valid for existing projects)
  | 'scale-switch' 
  | 'living-painting' 
  | 'b-roll-cutaway' 
  | 'none'

// Beat metadata for beat-matched establishing shots
export interface EstablishingShotBeat {
  beatNumber: number
  startPercent: number
  endPercent: number
  narrationText: string
  visualFocus: string
  shotType: 'wide' | 'medium' | 'close-up' | 'detail' | 'tracking'
  cameraMotion: string
  videoPrompt: string
}

export interface EstablishingShotSettings {
  enabled: boolean
  type: EstablishingShotType
  duration: number // 4-12 seconds
  useExistingFrame: boolean // Use scene's pre-generated frame
  // Beat-matched specific settings
  beats?: EstablishingShotBeat[]
  narrationDuration?: number
}

export type SceneSegmentAssetType = 'video' | 'image' | null

export type GenerationType = 'T2V' | 'I2V' | 'T2I' | 'UPLOAD'

// Veo 3.1 generation methods
export type VideoGenerationMethod = 'T2V' | 'I2V' | 'EXT' | 'FTV' | 'REF'

// ============================================================================
// Director's Console Types - Pre-Flight Workflow
// ============================================================================

/**
 * Approval status for segment video generation
 * Tracks user review workflow in Director's Console
 */
export type ApprovalStatus = 
  | 'auto-ready'      // System auto-configured, not yet user-reviewed
  | 'user-approved'   // User explicitly reviewed and approved settings
  | 'locked'          // User locked this take - prevents regeneration
  | 'rendering'       // Currently generating video
  | 'rendered'        // Video successfully generated
  | 'error'           // Generation failed

/**
 * Video generation configuration for a segment
 * Auto-drafted by useSegmentConfig hook, editable in DirectorDialog
 */
export interface VideoGenerationConfig {
  // Generation mode/method
  mode: VideoGenerationMethod
  
  // Prompts
  prompt: string              // Active prompt (switches based on mode)
  motionPrompt: string        // Motion-focused prompt for FTV mode
  visualPrompt: string        // Visual-focused prompt for I2V/T2V mode
  negativePrompt: string      // What to avoid in generation
  guidePrompt?: string        // Composed from audio/direction elements (narration, dialogue, music, sfx, direction)
  
  // Video parameters
  aspectRatio: '16:9' | '9:16'
  resolution: '720p' | '1080p'
  duration: number            // 4, 6, or 8 seconds
  
  // Asset URLs for generation
  startFrameUrl: string | null
  endFrameUrl: string | null
  sourceVideoUrl: string | null  // For EXT mode
  
  // Workflow status
  approvalStatus: ApprovalStatus
  confidence: number          // 0-100, auto-calculated confidence in settings
}

/**
 * Queue item for batch video rendering in Director's Console
 */
export interface DirectorQueueItem {
  segmentId: string
  sequenceIndex: number
  config: VideoGenerationConfig
  thumbnailUrl: string | null
  status: 'queued' | 'rendering' | 'complete' | 'error'
  error?: string
  progress?: number           // 0-100 during rendering
}

/**
 * Batch rendering options for Director's Console
 */
export interface BatchRenderOptions {
  mode: 'approved_only' | 'all' | 'selected'
  priority: 'sequence' | 'approved_first'
  delayBetween: number        // ms delay between API calls (rate limiting)
  selectedIds?: string[]      // Segment IDs to render when mode is 'selected'
}

// ============================================================================
// Generation Plan Types - Recommended generation strategy per segment
// ============================================================================

/**
 * Prerequisite status for a generation plan
 */
export interface GenerationPrerequisite {
  type: 'scene-image' | 'previous-frame' | 'character-ref' | 'veo-ref' | 'start-frame' | 'end-frame'
  label: string
  met: boolean
  required: boolean
  assetUrl?: string
}

/**
 * Generation Plan - Recommended strategy for generating a segment
 * Provides guidance for batch automation and user decision-making
 */
export interface GenerationPlan {
  // Primary recommendation
  recommendedMethod: VideoGenerationMethod
  // Confidence in the recommendation (0-100)
  confidence: number
  // Human-readable reasoning
  reasoning: string
  // Fallback if primary method fails or prerequisites not met
  fallbackMethod?: VideoGenerationMethod
  fallbackReason?: string
  // Prerequisites that must be met
  prerequisites: GenerationPrerequisite[]
  // Batch priority (lower = generate first)
  batchPriority: number
  // Estimated quality score if using this plan (0-100)
  qualityEstimate: number
  // Warnings or considerations
  warnings?: string[]
}

/**
 * Prompt context for staleness detection
 * Stores hashes of the source data used to generate the prompt
 */
export interface PromptContext {
  // Hash of dialogue lines covered by this segment
  dialogueHash: string
  // Hash of visual description at generation time
  visualDescriptionHash: string
  // Timestamp when prompt was generated
  generatedAt: string
  // Scene number at generation time
  sceneNumber?: number
  // Script version identifier (if available)
  scriptVersion?: string
}

export interface SceneSegmentTake {
  id: string
  createdAt: string
  assetUrl: string
  thumbnailUrl?: string
  durationSec?: number
  status: SceneSegmentStatus
  notes?: string
  // Veo video reference for extension - stores the Gemini Files API reference (e.g., "files/xxx")
  // This is needed for Veo video extension which only works with Veo-generated videos still in Gemini's system
  veoVideoRef?: string
}

export interface SceneSegmentReferences {
  startFrameUrl?: string | null
  endFrameUrl?: string | null
  // Enhanced reference data
  useSceneFrame?: boolean
  characterRefs?: string[] // Character names to use as references
  startFrameDescription?: string | null
  endFrameDescription?: string | null
  characterIds: string[]
  sceneRefIds: string[]
  objectRefIds: string[]
  
  // Keyframe State Machine - Frame Anchors
  startFrameAnchor?: FrameAnchor | null
  endFrameAnchor?: FrameAnchor | null
}

export interface SceneSegment {
  segmentId: string
  sequenceIndex: number
  startTime: number
  endTime: number
  status: SceneSegmentStatus
  generatedPrompt?: string
  userEditedPrompt?: string | null
  activeAssetUrl?: string | null
  assetType: SceneSegmentAssetType
  references: SceneSegmentReferences
  takes: SceneSegmentTake[]
  // Shot Metadata
  shotType?: string
  cameraAngle?: string
  cameraMovement?: string
  subject?: string
  action?: string
  transition?: 'cut' | 'dissolve' | 'fade_out'
  trigger?: string
  visualFrame?: string // e.g. "Shot Frame" url if separate from activeAssetUrl
  // Enhanced Veo 3.1 metadata
  generationMethod?: VideoGenerationMethod
  triggerReason?: string // Why we cut here (speaker change, action change, etc.)
  endFrameDescription?: string // Lookahead for next segment
  emotionalBeat?: string // The emotional intent of this segment
  // Phase 1: Character and Dialogue mapping for scene coverage
  characters?: SegmentCharacter[]  // Characters present in this segment
  dialogueLines?: SegmentDialogueLine[]  // Dialogue lines assigned to this segment
  dialogueLineIds?: string[]  // Phase 6: IDs of assigned dialogue lines (persisted to DB)
  // Phase 3: Keyframe settings for Ken Burns animation
  keyframeSettings?: SegmentKeyframeSettings
  // Establishing Shot metadata
  isEstablishingShot?: boolean
  establishingShotType?: EstablishingShotType
  shotNumber?: number // For B-Roll multi-shot sequences (1, 2, etc.)
  // Generation Plan - recommended strategy for batch automation
  generationPlan?: GenerationPlan
  // Prompt Context - for staleness detection when script changes
  promptContext?: PromptContext
  // Staleness flag - set when script changes invalidate the prompt
  isStale?: boolean
  // User instruction - general text instruction that overrides/augments prompts
  userInstruction?: string
  // Error message - stored when generation fails for detailed display
  errorMessage?: string
  
  // ============================================================================
  // Keyframe State Machine Fields
  // ============================================================================
  
  // Transition type: CONTINUE (inherit from previous) or CUT (fresh frame)
  transitionType?: TransitionType
  
  // Anchor status: tracks frame generation workflow state
  anchorStatus?: AnchorStatus
  
  // Action type: classified action for inverse proportionality weights
  actionType?: ActionType
  
  // Action prompt: describes what happens during this segment (for AI generation)
  actionPrompt?: string
  
  // Start frame URL (convenience accessor, also in references.startFrameUrl)
  startFrameUrl?: string | null
  
  // End frame URL (convenience accessor, also in references.endFrameUrl)
  endFrameUrl?: string | null
  
  // Production lock - prevents regeneration in Director's Console
  // Persisted to DB, survives page reloads
  lockedForProduction?: boolean
}

// Character presence in a segment
export interface SegmentCharacter {
  name: string
  role: 'speaking' | 'present' | 'background'
}

// Dialogue line assigned to a segment
export interface SegmentDialogueLine {
  id: string
  character: string
  line: string
  covered: boolean  // User confirms this dialogue is covered by the segment
}

// Ken Burns keyframe settings for manual animation control
export type KeyframeEasingType = 'smooth' | 'drift' | 'push' | 'dramatic'
export type KeyframePanDirection = 'none' | 'left' | 'right' | 'up' | 'down' | 'up-left' | 'up-right' | 'down-left' | 'down-right'

export interface SegmentKeyframeSettings {
  // Zoom keyframes (1.0 = no zoom, 1.2 = 20% zoom in)
  zoomStart: number  // 0.8 - 1.5
  zoomEnd: number    // 0.8 - 1.5
  // Pan keyframes (percentage offset from center, -50 to 50)
  panStartX: number
  panStartY: number
  panEndX: number
  panEndY: number
  // Easing for the transition
  easingType: KeyframeEasingType
  // Preset direction (overrides individual pan values)
  direction?: KeyframePanDirection
  // Whether to use auto-detected values or manual settings
  useAutoDetect: boolean
}

export interface SceneProductionData {
  isSegmented: boolean
  targetSegmentDuration: number
  segments: SceneSegment[]
  lastGeneratedAt?: string | null
}

export interface SceneProductionReferences {
  characters: any[]
  sceneReferences: VisualReference[]
  objectReferences: VisualReference[]
}

// ============================================================================
// V2 Audio Timeline Types - Multi-Language Support & Single Source of Truth
// ============================================================================

/**
 * Supported audio track types in the timeline
 */
export type AudioTrackType = 'voiceover' | 'description' | 'dialogue' | 'music' | 'sfx'

/**
 * Source of an audio clip - used for debugging and cleanup
 */
export type AudioClipSource = 'scene' | 'upload' | 'generated'

/**
 * Base audio clip definition (extends V1 AudioTrackClip for compatibility)
 */
export interface AudioTrackClipV2 {
  id: string
  url: string | null  // null = no audio, avoids stale URL references
  startTime: number   // In seconds, relative to scene start
  duration: number    // In seconds
  label?: string
  volume?: number     // 0-1
  trimStart?: number  // Offset from start of source
  trimEnd?: number    // Offset from end of source
  // V2 additions
  language: string    // 'en', 'es', 'th', etc.
  source: AudioClipSource
  scenePropertyPath?: string  // e.g., 'narrationAudio.en.url' for debugging
  characterName?: string      // For dialogue clips
  dialogueIndex?: number      // Index in scene.dialogue array
  // Stale audio warning
  isStale?: boolean           // True if audio doesn't match current dialogue
  staleReason?: string        // Human-readable reason for stale status
}

/**
 * Multi-language audio tracks - stores all languages, keyed by language code
 */
export interface MultiLanguageAudioTracks {
  [language: string]: AudioTracksDataV2
}

/**
 * Audio tracks for a single language
 */
export interface AudioTracksDataV2 {
  voiceover: AudioTrackClipV2 | null
  description: AudioTrackClipV2 | null
  dialogue: AudioTrackClipV2[]
  music: AudioTrackClipV2 | null
  sfx: AudioTrackClipV2[]
}

/**
 * Complete timeline audio state - tracks selected language and available options
 */
export interface TimelineAudioState {
  selectedLanguage: string
  availableLanguages: string[]
  tracks: MultiLanguageAudioTracks
  audioHash: string  // Hash of all URLs for change detection
}

/**
 * Props for SceneTimelineV2 component
 */
export interface SceneTimelineV2Props {
  segments: SceneSegment[]
  scene: any  // Scene object from script.script.scenes[idx]
  selectedSegmentId?: string
  selectedLanguage: string
  onLanguageChange: (language: string) => void
  onSegmentSelect: (segmentId: string) => void
  onPlayheadChange?: (time: number, segmentId?: string) => void
  onGenerateSceneMp4?: () => void
  onVisualClipChange?: (clipId: string, changes: { startTime?: number; duration?: number; trimStart?: number; trimEnd?: number }) => void
  onAudioClipChange?: (trackType: AudioTrackType, clipId: string, changes: { startTime?: number; duration?: number }) => void
  onAddSegment?: (afterSegmentId: string | null, duration: number) => void
  onDeleteSegment?: (segmentId: string) => void
  onReorderSegments?: (oldIndex: number, newIndex: number) => void
  onAddEstablishingShot?: () => void
  onAudioError?: (clipId: string, url: string) => void  // Handle 404s
  sceneFrameUrl?: string | null
  // Phase 2: Dialogue coverage indicators
  dialogueAssignments?: Record<string, Set<string>>
  // Phase 8: Audio alignment features
  onSegmentTimeChange?: (segmentId: string, newStartTime: number, newEndTime: number) => void
  onFitSegmentToDialogue?: (segmentId: string) => void
  onOpenSegmentPromptDialog?: (segmentId: string) => void
}

// ============================================================================
// Smart Prompt Video Editing Types - Constraint-Based UI for Veo 3.1
// ============================================================================

/**
 * Camera movement types supported by Veo 3.1
 */
export type CameraMovementType = 
  | 'static'
  | 'dolly-in'
  | 'pull-out'
  | 'pan-left'
  | 'pan-right'
  | 'tilt-up'
  | 'tilt-down'
  | 'crane'
  | 'handheld'
  | 'steadicam'
  | 'track'
  | 'orbit'
  | 'whip-pan'

/**
 * Shot framing types
 */
export type ShotFramingType =
  | 'extreme-wide'
  | 'wide'
  | 'medium-wide'
  | 'medium'
  | 'medium-close'
  | 'close-up'
  | 'extreme-close-up'
  | 'over-shoulder'
  | 'two-shot'
  | 'insert'

/**
 * Camera movement velocity
 */
export type CameraVelocity = 'slow' | 'medium' | 'fast'

/**
 * Focus behavior for camera
 */
export type FocusMode = 'locked' | 'rack' | 'follow' | 'deep'

/**
 * Camera & Temporal Control Settings
 * Module 1: Controls camera movement, framing, and temporal aspects
 */
export interface CameraControlSettings {
  // Movement
  movementType: CameraMovementType
  velocity: CameraVelocity
  // Framing
  shotFraming: ShotFramingType
  // Focus
  focusMode: FocusMode
  focusTarget?: string  // Character name or "subject"
  // Duration override (if different from segment duration)
  durationOverride?: number
  // Temporal controls
  motionIntensity: number  // 0-100, how much action/movement
  pacingStyle: 'contemplative' | 'natural' | 'dynamic' | 'frenetic'
}

/**
 * Lip-sync priority for performance
 */
export type LipSyncPriority = 'off' | 'loose' | 'tight'

/**
 * Eye contact behavior
 */
export type EyeContactMode = 'natural' | 'camera-aware' | 'avoid' | 'scene-specific'

/**
 * Performance & Dialog Sync Settings
 * Module 2: Controls actor performance and dialogue synchronization
 */
export interface PerformanceSettings {
  // Lip-sync (Phase 1 - Coming Soon placeholder)
  lipSyncEnabled: boolean
  lipSyncPriority: LipSyncPriority
  // Expression intensity (how much facial expression change)
  expressionIntensity: number  // 0-100
  // Micro-expressions toggle
  microExpressionsEnabled: boolean
  // Eye contact behavior
  eyeContactMode: EyeContactMode
  // Character focus (which character is primary in this segment)
  primaryCharacter?: string
  // Emotional state override
  emotionalState?: string  // "calm", "tense", "joyful", etc.
  // Body language intensity
  bodyLanguageIntensity: number  // 0-100
}

/**
 * Visual style presets
 */
export type VisualStylePreset = 
  | 'cinematic'
  | 'documentary'
  | 'commercial'
  | 'music-video'
  | 'horror'
  | 'romantic'
  | 'noir'
  | 'vintage'
  | 'custom'

/**
 * Lighting style
 */
export type LightingStyle = 
  | 'natural'
  | 'studio'
  | 'dramatic'
  | 'soft'
  | 'high-key'
  | 'low-key'
  | 'golden-hour'
  | 'blue-hour'
  | 'neon'
  | 'practical'

/**
 * Color grading preset
 */
export type ColorGradingPreset =
  | 'neutral'
  | 'warm'
  | 'cool'
  | 'teal-orange'
  | 'bleach-bypass'
  | 'vintage'
  | 'high-contrast'
  | 'desaturated'
  | 'vibrant'

/**
 * Visual Style Settings
 * Module 3: Controls look and feel of the generated video
 */
export interface VisualStyleSettings {
  // Style preset (quick selection)
  stylePreset: VisualStylePreset
  // Lighting
  lighting: LightingStyle
  lightingIntensity: number  // 0-100
  // Color grading
  colorGrading: ColorGradingPreset
  saturation: number  // 0-100, 50 = neutral
  contrast: number    // 0-100, 50 = neutral
  // Film grain / texture
  filmGrainEnabled: boolean
  filmGrainIntensity: number  // 0-100
  // Depth of field
  depthOfFieldEnabled: boolean
  apertureStyle: 'wide' | 'normal' | 'shallow'
  // Atmosphere
  atmosphereType?: 'none' | 'haze' | 'fog' | 'rain' | 'snow' | 'dust'
  atmosphereIntensity?: number  // 0-100
}

/**
 * Magic Edit selection method
 */
export type MagicEditSelectionMethod = 'auto' | 'mask' | 'prompt'

/**
 * Magic Edit operation type
 */
export type MagicEditOperationType = 
  | 'replace'      // Replace object with something else
  | 'remove'       // Remove object from scene
  | 'add'          // Add object to scene
  | 'modify'       // Modify existing object
  | 'style-transfer'  // Apply style to selection

/**
 * Magic Edit Settings (In-Painting / Object Manipulation)
 * Module 4: Targeted edits to specific regions or objects
 */
export interface MagicEditSettings {
  // Whether magic edit is enabled for this generation
  enabled: boolean
  // How the target is selected
  selectionMethod: MagicEditSelectionMethod
  // What to do with the selection
  operationType: MagicEditOperationType
  // The target description (what to select)
  targetDescription: string
  // What to do with the target
  changeDescription: string
  // Face preservation (important for character consistency)
  preserveFaces: boolean
  // Mask image URL (if using mask-based selection)
  maskUrl?: string
  // Blend mode for the edit
  blendStrength: number  // 0-100, how much the edit blends with original
}

/**
 * Combined settings for all Smart Prompt modules
 */
export interface SmartPromptSettings {
  camera: CameraControlSettings
  performance: PerformanceSettings
  visualStyle: VisualStyleSettings
  magicEdit: MagicEditSettings
}

/**
 * Default settings factory for Smart Prompt
 */
export const createDefaultSmartPromptSettings = (): SmartPromptSettings => ({
  camera: {
    movementType: 'static',
    velocity: 'medium',
    shotFraming: 'medium',
    focusMode: 'locked',
    motionIntensity: 50,
    pacingStyle: 'natural',
  },
  performance: {
    lipSyncEnabled: false,
    lipSyncPriority: 'off',
    expressionIntensity: 50,
    microExpressionsEnabled: true,
    eyeContactMode: 'natural',
    bodyLanguageIntensity: 50,
  },
  visualStyle: {
    stylePreset: 'cinematic',
    lighting: 'natural',
    lightingIntensity: 50,
    colorGrading: 'neutral',
    saturation: 50,
    contrast: 50,
    filmGrainEnabled: false,
    filmGrainIntensity: 0,
    depthOfFieldEnabled: true,
    apertureStyle: 'normal',
  },
  magicEdit: {
    enabled: false,
    selectionMethod: 'auto',
    operationType: 'modify',
    targetDescription: '',
    changeDescription: '',
    preserveFaces: true,
    blendStrength: 80,
  },
})

/**
 * Video Prompt Payload - Final output to Veo 3.1 API
 * This is what the prompt compiler produces from SmartPromptSettings
 */
export interface VideoPromptPayload {
  // Core prompt text (compiled from all settings)
  basePrompt: string
  // Negative prompt (things to avoid)
  negativePrompt: string
  // Generation method
  method: VideoGenerationMethod
  // Duration in seconds
  durationSeconds: number
  // Aspect ratio
  aspectRatio: '16:9' | '9:16' | '1:1'
  // Reference frames
  startFrameUrl?: string
  endFrameUrl?: string
  // Reference images (character/scene refs)
  referenceImages?: string[]
  // Video extension source
  sourceVideoRef?: string  // Veo video reference for extension
  // Control signals derived from SmartPromptSettings
  controlSignals: {
    cameraMovement: string
    cameraVelocity: string
    shotFraming: string
    motionIntensity: number
    lightingStyle: string
    colorGrading: string
    atmosphereType?: string
  }
  // Audio guidance (for lip-sync and dialog sync)
  audioGuidance?: {
    audioUrl?: string
    lipSyncEnabled: boolean
    syncPriority: LipSyncPriority
  }
  // Temporal consistency hints
  temporalConsistency: {
    preserveCharacters: string[]  // Character names to preserve
    preserveEnvironment: boolean
    previousSegmentUrl?: string   // For continuity
  }
  // Magic edit payload (if enabled)
  magicEditPayload?: {
    targetMask?: string
    targetPrompt: string
    changePrompt: string
    preserveFaces: boolean
    blendStrength: number
  }
}

// ============================================================================
// Voice Anchor Types - For Veo 3.1 consistent voice synthesis
// ============================================================================

/**
 * Narrator voice type presets for consistent voiceover generation
 * Used in Veo 3.1 prompts to describe the voice that should speak narration
 */
export type NarratorVoiceType = 
  | 'deep-masculine'       // Deep, authoritative male voice
  | 'warm-feminine'        // Warm, expressive female voice
  | 'neutral-documentary'  // Professional, neutral documentary style
  | 'elderly-wise'         // Aged, contemplative voice
  | 'young-energetic'      // Youthful, dynamic voice
  | 'custom'               // User-defined custom description

/**
 * Voice anchor for consistent voice generation in Veo 3.1
 * Based on the "Voice Anchor" concept from Veo prompt optimization guide
 */
export interface VoiceAnchor {
  type: NarratorVoiceType
  /** Custom description when type is 'custom' */
  customDescription?: string
  /** Additional voice modifiers (e.g., "British accent", "whispering") */
  modifiers?: string[]
}

/**
 * Voice anchor preset with display info and prompt text
 */
export interface VoiceAnchorPreset {
  type: NarratorVoiceType
  label: string
  description: string
  /** The actual text to include in Veo prompts */
  promptText: string
}

/**
 * Audio track selection for video playback overlay
 */
export interface SelectedAudioTracks {
  narration: boolean
  dialogue: boolean
  music: boolean
  sfx: boolean
}

/**
 * Audio track data for playback
 */
export interface AudioTrackData {
  type: 'narration' | 'dialogue' | 'music' | 'sfx'
  audioUrl?: string
  label: string
  enabled: boolean
}

